{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4497e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from SEMPIDataLoader import ListenerSpeakerFeatureDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys, os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f1e1f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"code\"))\n",
    "from metr import compute_ccc_batched , compute_pearson_correlation_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d60eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = ListenerSpeakerFeatureDataset(\n",
    "    csv_path=\"AudioVideo_Feature_Paths.csv\",\n",
    "    frame_length=64,\n",
    "    root_dir=\"./\",\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a942c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listener feature shape: torch.Size([329, 64])\n",
      "Speaker feature shape: torch.Size([329, 64])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "speaker_feat, listener_feat = sample[\"features\"]\n",
    "listener_dim, speaker_dim = listener_feat.shape[0], speaker_feat.shape[0]\n",
    "print(f\"Listener feature shape: {listener_feat.shape}\")\n",
    "print(f\"Speaker feature shape: {speaker_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2712818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, dropout, num_classes, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.activation_fn = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"tanh\": nn.Tanh()\n",
    "        }[config.activation_fn]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.activation_fn(x)\n",
    "        # if not (self.config.expnum in [1, 3, 4, 10]):\n",
    "        #     x = self.dropout(self.fc2(x))\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.fc3(x)\n",
    "        if self.config.extra_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c330cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ListenerSpeakerFusion(nn.Module):\n",
    "#     def __init__(self, config, listener_input_dim=329, listener_seq_len=50,\n",
    "#                  speaker_input_dim=768, speaker_seq_len=424,\n",
    "#                  reduced_speaker_dim=32):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "#         self.reduced_speaker_dim = reduced_speaker_dim\n",
    "#         self.listener_dim = listener_seq_len\n",
    "#         # self.ablation = config.ablation\n",
    "\n",
    "#         self.listener_pool = nn.Sequential(\n",
    "#             nn.Linear(listener_input_dim, 1),\n",
    "#             nn.Flatten(start_dim=1)\n",
    "#         )\n",
    "\n",
    "#         self.speaker_cnn = nn.Sequential(\n",
    "#             nn.Conv1d(speaker_input_dim, 64, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64, reduced_speaker_dim)\n",
    "#         )\n",
    "\n",
    "#         clf_input_size = 0\n",
    "#         # if self.ablation != 1: \n",
    "#         #     clf_input_size += self.listener_dim\n",
    "#         # if self.ablation != 2: \n",
    "#         #     clf_input_size += reduced_speaker_dim\n",
    "\n",
    "#         self.out = MLPClassifier(\n",
    "#             in_size=clf_input_size,\n",
    "#             hidden_size=config.hidden_size,\n",
    "#             dropout=config.dropout,\n",
    "#             num_classes=config.num_labels,\n",
    "#             config=config\n",
    "#         )\n",
    "\n",
    "#     def forward(self, listener_feats, speaker_feats):\n",
    "#         # listener_feats: [B, 50, 329]\n",
    "#         # speaker_feats: [B, 768, 424]\n",
    "#         fusion_vec = []\n",
    "        \n",
    "#         listener_x = self.listener_pool(listener_feats)  # [B, 50]\n",
    "#         fusion_vec.append(listener_x)\n",
    "#         speaker_y = self.speaker_cnn(speaker_feats)  # [B, 32]\n",
    "#         fusion_vec.append(speaker_y)\n",
    "\n",
    "#         # if self.ablation != 1:\n",
    "#         #     listener_x = self.listener_pool(listener_feats)  # [B, 50]\n",
    "#         #     fusion_vec.append(listener_x)\n",
    "\n",
    "#         # if self.ablation != 2:\n",
    "#         #     speaker_y = self.speaker_cnn(speaker_feats)  # [B, 32]\n",
    "#         #     fusion_vec.append(speaker_y)\n",
    "\n",
    "#         x = torch.cat(fusion_vec, dim=1)\n",
    "#         return self.out(x)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLPUpToHidden(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, dropout, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.activation_fn = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"Tanh\": nn.Tanh()\n",
    "        }[config.activation_fn]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.activation_fn(x)\n",
    "        return x  # no final output layer here\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=1, batch_first=True)\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        # q, k: [B, D]\n",
    "        q = q.unsqueeze(1)  # [B, 1, D]\n",
    "        k = k.unsqueeze(1)  # [B, 1, D]\n",
    "        v = k  # value = key\n",
    "        attn_out, _ = self.attn(q, k, v)  # [B, 1, D]\n",
    "        return attn_out.squeeze(1)  # [B, D]\n",
    "\n",
    "\n",
    "class ListenerSpeakerFusion(nn.Module):\n",
    "    def __init__(self, config, listener_input_dim=329, listener_seq_len=50,\n",
    "                 speaker_input_dim=768, speaker_seq_len=424,\n",
    "                 reduced_speaker_dim=32):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.listener_pool = nn.Sequential(\n",
    "            nn.Linear(listener_input_dim, 1),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "        self.speaker_cnn = nn.Sequential(\n",
    "            nn.Conv1d(speaker_input_dim, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, reduced_speaker_dim)\n",
    "        )\n",
    "\n",
    "        self.mlp_input_size = listener_seq_len + reduced_speaker_dim\n",
    "        self.mlp = MLPUpToHidden(\n",
    "            in_size=self.mlp_input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            dropout=config.dropout,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        self.cross_attention = CrossAttention(dim=config.hidden_size)\n",
    "        self.final_fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, listener_feats, speaker_feats):\n",
    "        # listener_feats: [B, 50, 329]\n",
    "        # speaker_feats: [B, 768, 424]\n",
    "\n",
    "        listener_x = self.listener_pool(listener_feats)  # [B, 50]\n",
    "        speaker_y = self.speaker_cnn(speaker_feats)      # [B, 32]\n",
    "\n",
    "        # Concatenate and pass through MLP (excluding final layer)\n",
    "        fused = torch.cat([listener_x, speaker_y], dim=1)  # [B, 82]\n",
    "        mlp_features = self.mlp(fused)                     # [B, hidden_size]\n",
    "\n",
    "        # Cross-attention between listener and speaker representations\n",
    "        attn_out = self.cross_attention(mlp_features, mlp_features)  # [B, hidden_size]\n",
    "\n",
    "        # Final prediction\n",
    "        return self.final_fc(attn_out)  # [B, num_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11363e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'activation_fn': 'Tanh',\n",
    "           'extra_dropout':0 ,\n",
    "             'hidden_size':128,\n",
    "             'dropout': 0.1,\n",
    "             'num_labels': 1}\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "sample = dataset[0]\n",
    "speaker_feat, listener_feat = sample[\"features\"]\n",
    "# Listener feature shape: torch.Size([50, 329])\n",
    "# Speaker feature shape: torch.Size([768, 424])\n",
    "\n",
    "model = ListenerSpeakerFusion(config=config , listener_input_dim=listener_feat.shape[1], listener_seq_len=listener_feat.shape[0], speaker_input_dim=speaker_feat.shape[0], speaker_seq_len=speaker_feat.shape[1], reduced_speaker_dim=32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ddfe60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListenerSpeakerFusion(\n",
       "  (listener_pool): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (speaker_cnn): Sequential(\n",
       "    (0): Conv1d(329, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveAvgPool1d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (mlp): MLPUpToHidden(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (fc1): Linear(in_features=361, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation_fn): Tanh()\n",
       "  )\n",
       "  (cross_attention): CrossAttention(\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25ee4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=1, batch_first=True)\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        query = query.unsqueeze(1)    # [B, 1, D]\n",
    "        context = context.unsqueeze(1)  # [B, 1, D]\n",
    "        attn_out, _ = self.attn(query, context, context)  # [B, 1, D]\n",
    "        return attn_out.squeeze(1)     # [B, D]\n",
    "\n",
    "\n",
    "class ListenerSpeakerHybridFusion(nn.Module):\n",
    "    def __init__(self, config, listener_input_dim=329, listener_seq_len=50,\n",
    "                 speaker_input_dim=768, speaker_seq_len=424, reduced_speaker_dim=32):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Listener: [B, 50, 329] → [B, 50]\n",
    "        self.listener_pool = nn.Sequential(\n",
    "            nn.Linear(listener_input_dim, 1),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "        # Speaker: [B, 768, 424] → [B, 32]\n",
    "        self.speaker_cnn = nn.Sequential(\n",
    "        nn.Conv1d(speaker_input_dim, reduced_speaker_dim, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool1d(1),\n",
    "        nn.Flatten(),\n",
    "        # nn.Linear(64, reduced_speaker_dim)     \n",
    "        )        \n",
    "\n",
    "        # MLP for fused early features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(listener_seq_len + reduced_speaker_dim, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp_norm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        # Project to hidden size before attention\n",
    "        self.listener_proj = nn.Linear(listener_seq_len, config.hidden_size)\n",
    "        self.speaker_proj = nn.Linear(reduced_speaker_dim, config.hidden_size)\n",
    "\n",
    "        # Cross-attention: listener attends to speaker\n",
    "        self.cross_attention = CrossAttention(dim=config.hidden_size)\n",
    "        self.attn_norm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, config.num_labels)\n",
    "        self.activation_fn = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"Tanh\": nn.Tanh()\n",
    "        }[config.activation_fn]\n",
    "\n",
    "    def forward(self, listener_feats, speaker_feats):\n",
    "        # Feature extraction\n",
    "        listener_x = self.listener_pool(listener_feats)     # [B, 50]\n",
    "        speaker_y = self.speaker_cnn(speaker_feats)         # [B, 32]\n",
    "\n",
    "        # MLP hidden fusion\n",
    "        fused = torch.cat([listener_x, speaker_y], dim=1)   # [B, 82]\n",
    "        mlp_hidden = self.mlp(fused)                        # [B, hidden_size]\n",
    "        mlp_hidden = self.mlp_norm(mlp_hidden)\n",
    "\n",
    "        \n",
    "        # Project listener/speaker before attention\n",
    "        listener_proj_out = self.listener_proj(listener_x)  # [B, hidden_size]\n",
    "        speaker_proj_out = self.speaker_proj(speaker_y)     # [B, hidden_size]\n",
    "\n",
    "        # Cross-attention: listener attends to speaker\n",
    "        listener_attn = self.cross_attention(listener_proj_out, speaker_proj_out)  # [B, hidden_size]\n",
    "        listener_attn = self.attn_norm(listener_attn)\n",
    "        \n",
    "        # Final fusion and classification\n",
    "        final_rep = torch.cat([mlp_hidden, listener_attn], dim=1)  # [B, hidden_size * 2]\n",
    "        final_rep = self.classifier(final_rep)  # [B, num_labels]\n",
    "        # print(\"final_rep shape: \", final_rep)\n",
    "        # print(\"Logits before tanh:\", final_rep.mean().item(), final_rep.min().item(), final_rep.max().item())\n",
    "\n",
    "        final_rep = self.activation_fn(final_rep)\n",
    "        return final_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43bf3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'activation_fn': 'Tanh',\n",
    "           'extra_dropout':0 ,\n",
    "             'hidden_size':128,\n",
    "             'dropout': 0.4,\n",
    "             'num_labels': 1}\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "sample = dataset[0]\n",
    "speaker_feat, listener_feat = sample[\"features\"]\n",
    "# Listener feature shape: torch.Size([50, 329])\n",
    "# Speaker feature shape: torch.Size([768, 424])\n",
    "\n",
    "model = ListenerSpeakerHybridFusion(config=config , listener_input_dim=listener_feat.shape[1], listener_seq_len=listener_feat.shape[0], speaker_input_dim=speaker_feat.shape[0], speaker_seq_len=speaker_feat.shape[1], reduced_speaker_dim=32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09705abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0157 | Val CCC: 0.0216 | Val PCC: 0.1383\n",
      "Epoch 2/20 - Loss: 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0156 | Val CCC: 0.0163 | Val PCC: 0.1358\n",
      "Epoch 3/20 - Loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0156 | Val CCC: 0.0298 | Val PCC: 0.1328\n",
      "Epoch 4/20 - Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0157 | Val CCC: 0.0214 | Val PCC: 0.1340\n",
      "Epoch 5/20 - Loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0156 | Val CCC: 0.0177 | Val PCC: 0.1248\n",
      "Epoch 6/20 - Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0160 | Val CCC: 0.0383 | Val PCC: 0.1324\n",
      "Epoch 7/20 - Loss: 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0156 | Val CCC: 0.0266 | Val PCC: 0.1195\n",
      "Epoch 8/20 - Loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0167 | Val CCC: 0.0141 | Val PCC: 0.0940\n",
      "Epoch 9/20 - Loss: 0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0165 | Val CCC: 0.0183 | Val PCC: 0.1083\n",
      "Epoch 10/20 - Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0157 | Val CCC: 0.0222 | Val PCC: 0.1148\n",
      "Epoch 11/20 - Loss: 0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0161 | Val CCC: 0.0414 | Val PCC: 0.1250\n",
      "Epoch 12/20 - Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0159 | Val CCC: 0.0199 | Val PCC: 0.0864\n",
      "Epoch 13/20 - Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0160 | Val CCC: 0.0230 | Val PCC: 0.0903\n",
      "Epoch 14/20 - Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0162 | Val CCC: 0.0333 | Val PCC: 0.1295\n",
      "Epoch 15/20 - Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0167 | Val CCC: 0.0335 | Val PCC: 0.1189\n",
      "Epoch 16/20 - Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0155 | Val CCC: 0.0410 | Val PCC: 0.1337\n",
      "Epoch 17/20 - Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0159 | Val CCC: 0.0444 | Val PCC: 0.1431\n",
      "Epoch 18/20 - Loss: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0162 | Val CCC: 0.0519 | Val PCC: 0.1623\n",
      "Epoch 19/20 - Loss: 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0162 | Val CCC: 0.0236 | Val PCC: 0.0882\n",
      "Epoch 20/20 - Loss: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Val Loss: 0.0156 | Val CCC: 0.0720 | Val PCC: 0.1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # or BCEWithLogitsLoss for binary\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        speaker_feat, listener_feat = batch[\"features\"]\n",
    "        engagement = batch[\"score\"]\n",
    "\n",
    "        speaker_feat = speaker_feat.to(device)\n",
    "        listener_feat = listener_feat.to(device)\n",
    "        engagement = engagement.to(device)\n",
    "        engagement = engagement.view(-1, 1)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        output = model(listener_feat, speaker_feat)\n",
    "        loss = criterion(output, engagement)\n",
    "        # print(output)\n",
    "\n",
    "        # print(engagement)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False):\n",
    "                speaker, listener = batch[\"features\"]\n",
    "                target = batch[\"score\"].to(device)\n",
    "                target = target.view(-1, 1)\n",
    "                speaker = speaker.to(device)\n",
    "                listener = listener.to(device)\n",
    "\n",
    "                output = model(listener, speaker)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item() * target.size(0)\n",
    "                val_preds.append(output.cpu())\n",
    "                val_targets.append(target.cpu())\n",
    "                # print(compute_ccc_batched(output.cpu(), target.cpu()))\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_ccc = compute_ccc_batched(\n",
    "            np.concatenate(val_preds),\n",
    "            np.concatenate(val_targets)\n",
    "            \n",
    "        )\n",
    "    val_pcc = compute_pearson_correlation_batched(\n",
    "             np.concatenate(val_preds),\n",
    "            np.concatenate(val_targets)\n",
    "           \n",
    "        )\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_targets = torch.cat(val_targets).numpy()\n",
    "      # if epoch == 5:\n",
    "        #     print(\"Val targets and preds:\")\n",
    "        #     print(val_targets , val_preds)\n",
    "        \n",
    "    print(f\"| Val Loss: {val_loss:.4f} | Val CCC: {val_ccc:.4f} | Val PCC: {val_pcc:.4f}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "edbb1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0287],\n",
       "        [-0.0269],\n",
       "        [-0.0198],\n",
       "        [-0.0165],\n",
       "        [-0.0175],\n",
       "        [-0.0072],\n",
       "        [-0.0110],\n",
       "        [-0.0126],\n",
       "        [-0.0389],\n",
       "        [-0.0078],\n",
       "        [-0.0236],\n",
       "        [-0.0324],\n",
       "        [-0.0485],\n",
       "        [ 0.0146],\n",
       "        [ 0.0105],\n",
       "        [ 0.0082],\n",
       "        [-0.0203],\n",
       "        [-0.0431],\n",
       "        [-0.0268],\n",
       "        [-0.0392],\n",
       "        [ 0.0051],\n",
       "        [-0.0114],\n",
       "        [-0.0369],\n",
       "        [-0.0213],\n",
       "        [ 0.0049],\n",
       "        [-0.0012],\n",
       "        [-0.0248],\n",
       "        [ 0.0065],\n",
       "        [ 0.0475],\n",
       "        [-0.0084],\n",
       "        [ 0.0036],\n",
       "        [-0.0120]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "140a3f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e-02],\n",
       "        [ 6.3333e-02],\n",
       "        [ 1.6333e-01],\n",
       "        [ 2.1000e-01],\n",
       "        [-2.7000e-01],\n",
       "        [-2.6667e-02],\n",
       "        [-9.0000e-02],\n",
       "        [ 9.6667e-02],\n",
       "        [-2.4333e-01],\n",
       "        [ 1.1333e-01],\n",
       "        [-1.5667e-01],\n",
       "        [ 8.0000e-02],\n",
       "        [-1.6667e-01],\n",
       "        [-1.1000e-01],\n",
       "        [ 1.9333e-01],\n",
       "        [ 1.9333e-01],\n",
       "        [ 9.0000e-02],\n",
       "        [ 1.9333e-01],\n",
       "        [ 1.7667e-01],\n",
       "        [-2.3667e-01],\n",
       "        [ 5.6667e-02],\n",
       "        [-3.4694e-18],\n",
       "        [-1.1000e-01],\n",
       "        [ 1.3333e-01],\n",
       "        [-2.0000e-02],\n",
       "        [ 7.3333e-02],\n",
       "        [ 1.9667e-01],\n",
       "        [ 1.3333e-02],\n",
       "        [ 1.9000e-01],\n",
       "        [ 1.0000e-02],\n",
       "        [ 1.3333e-02],\n",
       "        [-2.5333e-01]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c99981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
