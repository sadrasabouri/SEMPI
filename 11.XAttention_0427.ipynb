{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a8a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from SEMPIDataLoader import ListenerSpeakerFeatureDataset\n",
    "from multimodal_xattention import EarlyFusion\n",
    "\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1bafaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = ListenerSpeakerFeatureDataset(\n",
    "    csv_path=\"AudioVideo_Feature_Paths.csv\",\n",
    "    frame_length=64,\n",
    "    root_dir=\"./\",\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd9d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listener feature shape: torch.Size([329, 64])\n",
      "Speaker feature shape: torch.Size([424, 64])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "speaker_feat, listener_feat = sample[\"features\"]\n",
    "listener_dim, speaker_dim = listener_feat.shape[0], speaker_feat.shape[0]\n",
    "print(f\"Listener feature shape: {listener_feat.shape}\")\n",
    "print(f\"Speaker feature shape: {speaker_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f60b471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter extra_mlp.0.weight: dtype = torch.float64\n",
      "Parameter extra_mlp.0.bias: dtype = torch.float64\n",
      "Parameter out.fc1.weight: dtype = torch.float64\n",
      "Parameter out.fc1.bias: dtype = torch.float64\n",
      "Parameter out.fc2.weight: dtype = torch.float64\n",
      "Parameter out.fc2.bias: dtype = torch.float64\n",
      "Parameter out.fc3.weight: dtype = torch.float64\n",
      "Parameter out.fc3.bias: dtype = torch.float64\n",
      "Parameter cross_attention.in_proj_weight: dtype = torch.float64\n",
      "Parameter cross_attention.in_proj_bias: dtype = torch.float64\n",
      "Parameter cross_attention.out_proj.weight: dtype = torch.float64\n",
      "Parameter cross_attention.out_proj.bias: dtype = torch.float64\n",
      "Parameter audio_mlp.0.weight: dtype = torch.float64\n",
      "Parameter audio_mlp.0.bias: dtype = torch.float64\n",
      "Parameter fusion_mlp.0.weight: dtype = torch.float64\n",
      "Parameter fusion_mlp.0.bias: dtype = torch.float64\n",
      "EarlyFusion(\n",
      "  (extra_mlp): Sequential(\n",
      "    (0): Linear(in_features=329, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (out): Classifier(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (activation_fn): Tanh()\n",
      "  )\n",
      "  (cross_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (audio_mlp): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (fusion_mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'ckpt_root': './pretrained',\n",
    "    'activation_fn': 'tanh',\n",
    "    'extra_dropout': 0,\n",
    "    'hidden_size': 128,\n",
    "    'dropout': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'expnum': 8,\n",
    "    'openfacefeat': 1,\n",
    "    'openfacefeat_extramlp': 1,\n",
    "    'openfacefeat_extramlp_dim': 128,\n",
    "    'ablation': 8,\n",
    "    'num_labels': 1\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "\n",
    "model = EarlyFusion(config=config).to(device).double()\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name}: dtype = {param.dtype}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c17f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 289153\n",
      "Number of trainable parameters: 289153\n"
     ]
    }
   ],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "586755f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ccc_batched(y_pred, y_true):\n",
    "    # Flatten \n",
    "    y_true_np = y_true.flatten()\n",
    "    y_pred_np = y_pred.flatten()\n",
    "    mean_true = np.mean(y_true_np)\n",
    "    mean_pred = np.mean(y_pred_np)\n",
    "    std_true = np.std(y_true_np)\n",
    "    std_pred = np.std(y_pred_np)\n",
    "\n",
    "    # Pearson\n",
    "    rho, _ = pearsonr(y_true_np, y_pred_np)\n",
    "\n",
    "    # CCC from Pearson\n",
    "    ccc = (2 * rho * std_true * std_pred) / (std_true**2 + std_pred**2 + (mean_true - mean_pred)**2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_pearson_correlation_batched(y_pred, y_true):\n",
    "    # Flatten \n",
    "    y_true_np = y_true.flatten()\n",
    "    y_pred_np = y_pred.flatten()\n",
    "\n",
    "    # Calculate Pearson \n",
    "    rho, _ = pearsonr(y_true_np, y_pred_np)\n",
    "\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "586f3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training settings\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "early_stop_patience = 10\n",
    "best_model_path = 'best_early_fusion_model.pth'\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer with your settings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Training tracking\n",
    "best_val_loss = float('inf')\n",
    "no_improve_count = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_cccs = []\n",
    "val_pccs = []\n",
    "\n",
    "# Create figure for real-time plotting\n",
    "plt.figure(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5ff922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_feat dtype before conversion: torch.float32\n",
      "listener_feat dtype before conversion: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x21056 and 768x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     listener_feat_list\u001b[38;5;241m.\u001b[39mappend(listener_feat[i]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass with properly formatted features\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenfacefeat_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlistener_feat_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, engagement)\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\25 Spring\\CSCI535\\Project\\SEMPI-main\\multimodal_xattention.py:251\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, audio_paths, openfacefeat_)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenfacefeat dtype after cuda:\u001b[39m\u001b[38;5;124m\"\u001b[39m, openfacefeat\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    250\u001b[0m openfacefeat \u001b[38;5;241m=\u001b[39m (openfacefeat \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_VEC) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_VEC \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_VEC)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenfacefeat dtype after normalization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, openfacefeat\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    252\u001b[0m openfacefeat \u001b[38;5;241m=\u001b[39m openfacefeat \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    254\u001b[0m openfacefeat_filtered \u001b[38;5;241m=\u001b[39m openfacefeat[:, [OPENFACE_COLUMN_NAMES\u001b[38;5;241m.\u001b[39mindex(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilteredcolumns]]\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x21056 and 768x128)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False):\n",
    "            # Get features and target\n",
    "            speaker_feat, listener_feat = batch[\"features\"]\n",
    "            engagement = batch[\"score\"]\n",
    "            print(f\"speaker_feat dtype before conversion: {speaker_feat.dtype}\")\n",
    "            print(f\"listener_feat dtype before conversion: {listener_feat.dtype}\")\n",
    "            \n",
    "            # Move to device and reshape\n",
    "            speaker_feat = speaker_feat.to(device).double()\n",
    "            listener_feat = listener_feat.to(device).double()\n",
    "            engagement = engagement.to(device)\n",
    "            engagement = engagement.view(-1, 1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create a list of listener features with transposed dimensions\n",
    "            listener_feat_list = []\n",
    "            for i in range(listener_feat.size(0)):\n",
    "                listener_feat_list.append(listener_feat[i].transpose(0, 1))\n",
    "            \n",
    "            # Forward pass with properly formatted features\n",
    "            output = model(audio_paths=speaker_feat, openfacefeat_=listener_feat_list)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, engagement)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False):\n",
    "                # Get features and target\n",
    "                speaker_feat, listener_feat = batch[\"features\"]\n",
    "                engagement = batch[\"score\"]\n",
    "                \n",
    "                # Move to device and reshape\n",
    "                speaker_feat = speaker_feat.to(device)\n",
    "                listener_feat = listener_feat.to(device)\n",
    "                engagement = engagement.to(device)\n",
    "                engagement = engagement.view(-1, 1)\n",
    "\n",
    "                # Create a list of listener features with transposed dimensions\n",
    "                listener_feat_list = []\n",
    "                for i in range(listener_feat.size(0)):\n",
    "                    listener_feat_list.append(listener_feat[i].transpose(0, 1))\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(audio_paths=speaker_feat, openfacefeat_=listener_feat_list)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, engagement)\n",
    "                \n",
    "                # Track loss\n",
    "                val_loss += loss.item() * engagement.size(0)\n",
    "                \n",
    "                # Store predictions and targets for metrics\n",
    "                val_preds.append(output.cpu())\n",
    "                val_targets.append(engagement.cpu())\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_preds_combined = torch.cat(val_preds).numpy()\n",
    "        val_targets_combined = torch.cat(val_targets).numpy()\n",
    "        \n",
    "        val_ccc = compute_ccc_batched(val_preds_combined, val_targets_combined)\n",
    "        val_pcc = compute_pearson_correlation_batched(val_preds_combined, val_targets_combined)\n",
    "        \n",
    "        val_cccs.append(val_ccc)\n",
    "        val_pccs.append(val_pcc)\n",
    "        \n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, CCC: {val_ccc:.4f}, PCC: {val_pcc:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_ccc': val_ccc,\n",
    "                'val_pcc': val_pcc,\n",
    "                'config': config,\n",
    "            }, best_model_path)\n",
    "            print(f\"✓ Saved new best model with validation loss: {avg_val_loss:.4f}\")\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            print(f\"✗ No improvement for {no_improve_count} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_count >= early_stop_patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Update plots\n",
    "        plt.clf()\n",
    "        \n",
    "        # Plot 1: Loss curves\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Curves')\n",
    "        \n",
    "        # Plot 2: CCC curve\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(val_cccs, label='Validation CCC', color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('CCC')\n",
    "        plt.legend()\n",
    "        plt.title('Concordance Correlation Coefficient')\n",
    "        \n",
    "        # Plot 3: PCC curve\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(val_pccs, label='Validation PCC', color='purple')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('PCC')\n",
    "        plt.legend()\n",
    "        plt.title('Pearson Correlation Coefficient')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.pause(0.1)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec09c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "# Plot 2: CCC curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_cccs, label='Validation CCC', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CCC')\n",
    "plt.legend()\n",
    "plt.title('Concordance Correlation Coefficient')\n",
    "\n",
    "# Plot 3: PCC curve\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_pccs, label='Validation PCC', color='purple')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PCC')\n",
    "plt.legend()\n",
    "plt.title('Pearson Correlation Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_training_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453aa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for evaluation\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with:\")\n",
    "    print(f\"- Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"- Validation CCC: {checkpoint['val_ccc']:.4f}\")\n",
    "    print(f\"- Validation PCC: {checkpoint['val_pcc']:.4f}\")\n",
    "\n",
    "# Comprehensive evaluation on validation set\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            speaker_feat, listener_feat = batch[\"features\"]\n",
    "            engagement = batch[\"score\"]\n",
    "            \n",
    "            speaker_feat = speaker_feat.to(device)\n",
    "            listener_feat = listener_feat.to(device)\n",
    "            engagement = engagement.to(device)\n",
    "            engagement = engagement.view(-1, 1)\n",
    "            \n",
    "            output = model(listener_feat, speaker_feat)\n",
    "            \n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(engagement.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((all_targets - all_preds) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    ccc = compute_ccc_batched(all_preds, all_targets)\n",
    "    pcc = compute_pearson_correlation_batched(all_preds, all_targets)\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'ccc': ccc,\n",
    "        'pcc': pcc,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# Run final evaluation\n",
    "print(\"Running final evaluation on validation set...\")\n",
    "eval_results = evaluate_model(model, val_loader, device)\n",
    "\n",
    "print(\"\\nFinal Evaluation Metrics:\")\n",
    "print(f\"- MSE: {eval_results['mse']:.4f}\")\n",
    "print(f\"- RMSE: {eval_results['rmse']:.4f}\")\n",
    "print(f\"- CCC: {eval_results['ccc']:.4f}\")\n",
    "print(f\"- PCC: {eval_results['pcc']:.4f}\")\n",
    "\n",
    "# Plot predictions vs targets for a sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "sample_size = min(100, len(eval_results['predictions']))\n",
    "indices = np.random.choice(len(eval_results['predictions']), sample_size, replace=False)\n",
    "\n",
    "plt.scatter(eval_results['targets'][indices], eval_results['predictions'][indices], alpha=0.5)\n",
    "plt.plot([min(eval_results['targets']), max(eval_results['targets'])], \n",
    "         [min(eval_results['targets']), max(eval_results['targets'])], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.grid(True)\n",
    "plt.savefig('prediction_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2168a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23107639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda\\envs\\eng_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m val_pccs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create figure for real-time plotting\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     28\u001b[0m fig, (ax1, ax2, ax3) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407fcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
