{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SEMPIDataLoader import InterPersenSEMPIDataset, DataSetLoader, DataLoader\n",
    "from SEMPIDataLoader import create_dataloaders\n",
    "from SEMPIDataLoader import DATA_PATH\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 96\n",
      "15256\n",
      "Data loaded successfully!\n",
      "Train size: 12204\n",
      "Val size: 3052\n",
      "Batch 0\n",
      "torch.Size([32, 2, 329, 64])\n",
      "tensor([[7, 2],\n",
      "        [4, 5],\n",
      "        [4, 1],\n",
      "        [8, 4],\n",
      "        [8, 6],\n",
      "        [3, 4],\n",
      "        [4, 2],\n",
      "        [4, 5],\n",
      "        [2, 4],\n",
      "        [4, 6],\n",
      "        [2, 1],\n",
      "        [6, 4],\n",
      "        [2, 5],\n",
      "        [1, 2],\n",
      "        [5, 7],\n",
      "        [1, 5],\n",
      "        [3, 1],\n",
      "        [6, 2],\n",
      "        [5, 7],\n",
      "        [1, 4],\n",
      "        [4, 3],\n",
      "        [1, 4],\n",
      "        [4, 5],\n",
      "        [4, 1],\n",
      "        [1, 8],\n",
      "        [1, 5],\n",
      "        [8, 7],\n",
      "        [5, 1],\n",
      "        [3, 6],\n",
      "        [8, 4],\n",
      "        [1, 5],\n",
      "        [5, 4]], dtype=torch.int32)\n",
      "tensor([ 1.6667e-02, -1.6667e-02,  1.5000e-01,  5.0000e-02, -1.3333e-01,\n",
      "         8.3333e-02, -1.3878e-17,  1.8333e-01, -1.8333e-01, -3.6667e-01,\n",
      "         1.8333e-01,  1.8333e-01, -1.0000e-01,  1.6667e-01, -2.7756e-17,\n",
      "         1.6667e-02, -5.0000e-02,  5.0000e-02, -6.6667e-02, -1.6667e-02,\n",
      "         5.0000e-02, -6.6667e-02, -6.6667e-02,  1.6667e-01, -2.7756e-17,\n",
      "        -3.6667e-01,  5.0000e-02, -1.0000e-01, -8.3333e-02,  1.5000e-01,\n",
      "        -3.0000e-01, -3.0000e-01])\n",
      "Batch 1\n",
      "torch.Size([32, 2, 329, 64])\n",
      "tensor([[2, 7],\n",
      "        [4, 5],\n",
      "        [3, 4],\n",
      "        [1, 5],\n",
      "        [8, 2],\n",
      "        [2, 5],\n",
      "        [3, 2],\n",
      "        [1, 5],\n",
      "        [1, 5],\n",
      "        [4, 7],\n",
      "        [5, 2],\n",
      "        [1, 3],\n",
      "        [4, 6],\n",
      "        [4, 2],\n",
      "        [2, 6],\n",
      "        [8, 7],\n",
      "        [6, 2],\n",
      "        [3, 5],\n",
      "        [3, 4],\n",
      "        [1, 7],\n",
      "        [2, 5],\n",
      "        [1, 4],\n",
      "        [5, 2],\n",
      "        [3, 5],\n",
      "        [4, 1],\n",
      "        [5, 3],\n",
      "        [3, 1],\n",
      "        [2, 6],\n",
      "        [8, 2],\n",
      "        [4, 5],\n",
      "        [3, 1],\n",
      "        [5, 8]], dtype=torch.int32)\n",
      "tensor([-8.3333e-02,  3.3333e-02, -1.6667e-01, -3.3333e-02,  1.5000e-01,\n",
      "        -5.0000e-02,  6.6667e-02,  1.6667e-02, -3.3333e-02, -1.6667e-02,\n",
      "        -1.6667e-01,  8.3333e-02,  6.6667e-02,  8.3333e-02, -1.0000e-01,\n",
      "         1.6667e-02, -3.0000e-01, -2.7756e-17,  1.1667e-01,  1.6667e-02,\n",
      "        -3.3333e-02,  1.0000e-01, -1.6667e-01, -1.1667e-01,  6.6667e-02,\n",
      "         5.0000e-02,  5.0000e-02, -1.3333e-01,  5.0000e-02,  1.6667e-01,\n",
      "         1.1667e-01, -2.7756e-17])\n",
      "Batch 2\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and dataloaders with pickle\n",
    "with open(os.path.join(DATA_PATH, 'dataset.pkl'), 'rb') as f:\n",
    "    dataset: InterPersenSEMPIDataset = pickle.load(f)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(dataset, batch_size=32)\n",
    "print(len(train_loader), len(val_loader))\n",
    "print(len(dataset))\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Train size: {len(train_loader.dataset)}\")\n",
    "print(f\"Val size: {len(val_loader.dataset)}\")\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    print(f\"Batch {i}\")\n",
    "    if i == 2:\n",
    "        break\n",
    "    print(data['features'].shape)\n",
    "    print(data['pids'])\n",
    "    print(data['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, main_input, attending_input):\n",
    "        \"\"\"\n",
    "        main_input: (batch_size, num_features, num_frames)\n",
    "        attending_input: (batch_size, num_features, num_frames)\n",
    "        \"\"\"\n",
    "        main_input = main_input.permute(0, 2, 1)  # (batch_size, num_frames, num_features)\n",
    "        attending_input = attending_input.permute(0, 2, 1)  # (batch_size, num_frames, num_features)\n",
    "        attended_output, attention_weights = self.attention(main_input, attending_input, attending_input)\n",
    "        return attended_output.permute(0, 2, 1)  # (batch_size, num_features, num_frames)\n",
    "\n",
    "\n",
    "class EngagementPredictor(nn.Module):\n",
    "    def __init__(self, num_features, num_frames, hidden_dim=128, num_heads=4):\n",
    "        super(EngagementPredictor, self).__init__()\n",
    "        self.cross_attention = CrossAttention(feature_dim=num_features, num_heads=num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_features * num_frames, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        main_input, attending_input = x[:, 0, :, :], x[:, 1, :, :]\n",
    "        attended_output = self.cross_attention(main_input, attending_input)\n",
    "        flattened = attended_output.contiguous().reshape(attended_output.size(0), -1) # Flatten (batch_size, num_features * num_frames)\n",
    "        return self.mlp(flattened).squeeze(-1)  # Output a single value per sample\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            feat = batch['features'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feat)\n",
    "            loss = criterion(predictions, batch['score'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {total_loss / len(train_loader)}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                feat = batch['features'].to(device)\n",
    "                predictions = model(feat)\n",
    "                loss = criterion(predictions, batch['score'].to(device))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss = {val_loss / len(val_loader)}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.0101529877847402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:10<01:34, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 2: Training Loss = 1.0099812193690794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:20<01:20, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 3: Training Loss = 1.0098998749443373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:30<01:09,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 4: Training Loss = 1.010009327328018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:40<00:59,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 5: Training Loss = 1.0101095759431729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:49<00:49,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 6: Training Loss = 1.009867258408931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:59<00:39,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 7: Training Loss = 1.010077150860382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:09<00:29,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 8: Training Loss = 1.0099845151002493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:19<00:19,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 9: Training Loss = 1.0099701670764005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:29<00:09,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Epoch 10: Training Loss = 1.0101879359227826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:39<00:00,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.0103347015877564\n",
      "Training completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 329\n",
    "num_frames = 64\n",
    "\n",
    "model = train_model(EngagementPredictor(num_features, num_frames, num_heads=7), # it must be divisor of 329\n",
    "            train_loader, val_loader, num_epochs=10, lr=1e-4, device='cpu')\n",
    "print(\"Training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "torch.Size([32, 2, 329, 64])\n",
      "tensor([[1, 4],\n",
      "        [3, 5],\n",
      "        [1, 5],\n",
      "        [8, 1],\n",
      "        [4, 1],\n",
      "        [1, 4],\n",
      "        [5, 2],\n",
      "        [4, 3],\n",
      "        [7, 2],\n",
      "        [2, 4],\n",
      "        [2, 6],\n",
      "        [5, 6],\n",
      "        [6, 5],\n",
      "        [6, 1],\n",
      "        [5, 1],\n",
      "        [5, 3],\n",
      "        [2, 4],\n",
      "        [4, 1],\n",
      "        [6, 7],\n",
      "        [3, 6],\n",
      "        [6, 5],\n",
      "        [3, 5],\n",
      "        [4, 1],\n",
      "        [4, 3],\n",
      "        [4, 2],\n",
      "        [4, 1],\n",
      "        [2, 4],\n",
      "        [2, 1],\n",
      "        [2, 1],\n",
      "        [3, 4],\n",
      "        [5, 3],\n",
      "        [6, 4]], dtype=torch.int32)\n",
      "tensor([ 8.3333e-02, -2.6667e-01,  3.3333e-02,  5.0000e-02, -3.6667e-01,\n",
      "        -1.0000e-01,  3.3333e-02,  8.3333e-02, -8.3333e-02, -2.7756e-17,\n",
      "        -3.8333e-01, -1.3333e-01,  1.6667e-01,  2.0000e-01,  6.6667e-02,\n",
      "         3.3333e-02, -2.6667e-01,  1.1667e-01,  1.6667e-01, -2.7756e-17,\n",
      "         2.0000e-01,  1.8333e-01, -1.6667e-01, -3.3333e-02,  5.0000e-02,\n",
      "         5.0000e-02, -2.8333e-01,  5.0000e-02, -2.7756e-17, -1.0000e-01,\n",
      "         5.0000e-02,  2.1667e-01])\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Batch 1\n",
      "torch.Size([32, 2, 329, 64])\n",
      "tensor([[2, 5],\n",
      "        [1, 6],\n",
      "        [3, 5],\n",
      "        [2, 5],\n",
      "        [3, 7],\n",
      "        [3, 5],\n",
      "        [4, 1],\n",
      "        [1, 4],\n",
      "        [4, 3],\n",
      "        [4, 5],\n",
      "        [2, 5],\n",
      "        [5, 1],\n",
      "        [7, 5],\n",
      "        [6, 5],\n",
      "        [7, 2],\n",
      "        [2, 3],\n",
      "        [7, 6],\n",
      "        [3, 1],\n",
      "        [7, 8],\n",
      "        [6, 5],\n",
      "        [7, 1],\n",
      "        [3, 1],\n",
      "        [4, 1],\n",
      "        [4, 5],\n",
      "        [3, 7],\n",
      "        [3, 5],\n",
      "        [1, 3],\n",
      "        [4, 3],\n",
      "        [3, 2],\n",
      "        [4, 2],\n",
      "        [4, 7],\n",
      "        [1, 5]], dtype=torch.int32)\n",
      "tensor([-1.3333e-01,  1.3333e-01,  8.3333e-02, -1.6667e-01,  1.1667e-01,\n",
      "         1.1667e-01,  5.0000e-02,  1.3333e-01,  5.0000e-02, -3.8333e-01,\n",
      "         1.8333e-01, -5.0000e-02,  8.3333e-02,  5.0000e-02, -1.5000e-01,\n",
      "         1.3333e-01, -3.3333e-02,  3.3333e-02, -1.6667e-02, -2.3333e-01,\n",
      "        -3.3333e-02,  6.6667e-02,  3.3333e-02,  3.3333e-02,  1.1667e-01,\n",
      "         1.8333e-01,  6.6667e-02,  8.3333e-02, -6.6667e-02,  6.9389e-18,\n",
      "         2.0000e-01,  1.8333e-01])\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Batch 2\n"
     ]
    }
   ],
   "source": [
    "# see models's prediction on a batch\n",
    "for i, data in enumerate(val_loader):\n",
    "    print(f\"Batch {i}\")\n",
    "    if i == 2:\n",
    "        break\n",
    "    print(data['features'].shape)\n",
    "    print(data['pids'])\n",
    "    print(data['score'])\n",
    "    print(model(data['features']).detach().numpy())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'engagement_predictor_xatn.pth')\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = EngagementPredictor(num_features, num_frames, num_heads=7)\n",
    "model.load_state_dict(torch.load('engagement_predictor_xatn.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EngagementPredictor(\n",
       "  (cross_attention): CrossAttention(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=329, out_features=329, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=21056, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
